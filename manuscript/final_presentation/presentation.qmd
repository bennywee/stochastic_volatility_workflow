---
title: "Comparison of MCMC algorithms in Stochastic Volatility Models using Simulation Based Calibration"
format: 
    beamer:
        classoption:
            - '`xcolor={dvipsnames}`{=latex}' 
        header-includes: |
            \setbeamertemplate{itemize items}[circle]
author: "By Benjamin Wee"
institute: "Supervised by Catherine Forbes and Lauren Kennedy"
---

## Research Outline
- Stochastic Volatility model
- MCMC
    
    + Hamiltonian Monte Carlo
        
    + Kim Shephard & Chib (1998) bespoke MCMC strategy

- Compare performance of MCMC
        
    + Simulation based Calibration: check correct posterior estimates from known data generating process

    + \color{gray}{Efficiency: Effective sample size}

- Model reparameterisation - improving MCMC performance

- \color{gray}{Kim Shephard \& Chib Importance Weights - correcting approximation error}

## Log returns S&P 500
![]("returns.png")

<!-- Suppose you're an analyst looking at this time series. Notice hetero behaviour/volatility clustering. Could try to model with SV -->

## Stochastic Volatility model
KSC (1998) estimate a univariate discrete time SV model which models the variance as a latent stochastic process.

$$
\begin{aligned}
y_t =& \space \beta e^{h_t/2} \epsilon_t \\
h_{t+1} =& \space \mu +\phi(h_t - \mu) + \sigma_{\eta} \eta_t  \\
h_1 \sim&  N\left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
\epsilon_t \sim& N(0,1) \quad \eta_t \sim N(0,1) \\
\end{aligned}
$$

- State space model
- Complex! More parameters/unknowns then data points
- Estimate Using Bayesian methods

## Estimating the model on real data
 
. . .

<!-- Keen analyst wants to estimate the model straight away. 2 MCMC methods.-->
<!-- These are your posterior estimates. Which one is correct?-->
<!-- Don't care about prediction yet. Care about accurate parameter estimates -->

![]("../motivating_example/real_data_ex.png")

## Estimate model on simulated data
 
. . .

<!--Benefit of Bayesian model is that they're generative. Model can simulate data based on known parameter values.-->
<!-- Pick a value for parameteres, simulate the data, estimate the model on simulated data. See if you can capture true parameters -->
<!-- Phi not good! The model/algorithm doesn't estimate phi very well. -->
<!-- Not true. 5% chance that any random draw from the true probabiltiy dist will fall outside the 95% interval.-->
<!-- Gen models are great because we can simulate data from the process implied by the model conditional on parameters. -->

![]("../motivating_example/single_sim.png")

## Research goal
- Real data
    + Don't know much posterior estimate is correct
    + Confounding causes if convergence diagnostics fail (incorrect model, poor MCMC choice)

. . .

- Single simulation
    + Incorrect analysis may produce correct result by chance
    + Correct analysis may produce poor result by chance

. . .

**Research objective**

- Validate the computational methods used to estimate Bayesian stochastic volatility (SV) models

- Design a simulation study to check of correct posterior estimates are returned

- Compare Hamiltonian Monte Carlo with KSC (1998), which MCMC returns calibrated estimates?

# Simulation Based Calibration



## Simulation Based Calibration (SBC)
Let $\theta$ be a parameter and $y$ represent the dataset. Start with draw a sample from the prior distribution:
$$
\begin{aligned}
\theta^{sim} \sim \pi(\theta)
\end{aligned}
$$

. . .

Generate a dataset given by the prior draws.

$$
\begin{aligned}
y^{sim} \sim \pi (y|\theta^{sim})
\end{aligned}
$$

. . .

Then take draws from the posterior distribution generated by a MCMC algorithm or estimation strategy (Hamiltonian Monte Carlo or Gaussian approximation in our case).

$$
\begin{aligned}
\{\theta_1,\dots , \theta_{L}\} \sim \pi (\theta | y^{sim})
\end{aligned}
$$


## Simulation Based Calibration (SBC)
The rank statistic for a given parameter and simulation follows a discrete uniform distribution (proven in Talts et al. (2018)).

$$
\begin{aligned}
r = rank(\{\theta_1,\dots , \theta_{L}\}, \theta^{sim}) = \sum_{l=1}^{L}1[\theta_{l} < \theta^{sim}]
\end{aligned}
$$

. . .

Therefore, if the distribution of rank statistics deviates from a discrete uniform distribution, there may be evidence that our analysis or computation is not returning the correct posteriors (not calibrated). 

. . .

If Bayesian computation is well calibrated, then the posterior (credible) intervals will have sufficient coverage. That is, for any percentage interval selected (for example 90%) then 90\% of these constructed intervals will contain $\theta^{sim}$.

# Markov Chain Monte Carlo (MCMC) algorithms

## Markov Chain Monte Carlo (MCMC) algorithms

**Kim, Shephard, Chib (1998)**

$$
\begin{aligned}
y_t &= \space e^{h_t/2} \epsilon_t \\
y_t^{*} &= log(y_t^2) = h_t + z_t\\ 
\end{aligned}
$$

- Approximate log chi squared error using Gaussian mixture model
- Kalman Filter
- Conjugate posteriors
- Metropolis Hastings

. . .

**Hamiltonian Monte Carlo**

- Own programming language. Flexibly estimate any model

- Uses the gradients of the target posterior distribution to generate an efficient path for the sampler to explore. 

- Allows for direct sampling of the specified stochastic volatility model

# Results

## Results
- 5000 SBC iterations
- Simulated datasets of 1000 (which means 1000 latent states)
- 999 posterior draws for HMC (1000 rank statistics per parameter)
- 9,999 posterior draws for KSC (10,000 rank statistics per parameter)

## HMC (5000 Iterations)
![]("../results/hmc_cp_5k.png")

## KSC (5000 Iterations)
![]("../results/ksc_cp_5k.png")

# Model reparameterisation

## Reparameterised HMC (5000 Iterations)
![]("../results/hmc_ncp_5k.png")

## Reparameterised KSC (5000 Iterations)
![]("../results/ksc_ncp_5k.png")

## Interpretation of miscalibration

## Discussion & Limitations
- Reparameterised model with HMC performs best based on calibration

- SBC is computationally intensive (required use of Monash HPC Cluster)

- What constitutes a fair comparison? Reparameterised HMC calibrated after 999 draws.


# Questions