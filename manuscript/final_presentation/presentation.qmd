---
title: "Comparison of MCMC algorithms in Stochastic Volatility Models using Simulation Based Calibration"
format: 
    beamer:
        classoption:
            - '`xcolor={dvipsnames}`{=latex}' 
        header-includes: |
            \setbeamertemplate{itemize items}[circle]
author: "By Benjamin Wee"
institute: "Supervised by Catherine Forbes and Lauren Kennedy"
---

## Research Outline
- Stochastic Volatility model
- MCMC
    
    + Hamiltonian Monte Carlo
        
    + Kim Shephard & Chib (1998) bespoke MCMC strategy

- Compare performance of MCMC
        
    + Simulation based Calibration: check correct posterior estimates from known data generating process

    + \color{gray}{Efficiency: Effective sample size}

- Model reparameterisation - improving MCMC performance

- \color{gray}{Kim Shephard \& Chib Importance Weights - correcting approximation error}

## Log returns S&P 500
![]("returns.png")

<!-- Suppose you're an analyst looking at this time series. Notice hetero behaviour/volatility clustering. Could try to model with SV -->

## Stochastic Volatility model
KSC (1998) estimate a univariate discrete time SV model which models the variance as a latent stochastic process.

$$
\begin{aligned}
y_t =& \space \beta e^{h_t/2} \epsilon_t \\
h_{t+1} =& \space \mu +\phi(h_t - \mu) + \sigma_{\eta} \eta_t  \\
h_1 \sim&  N\left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
\epsilon_t \sim& N(0,1) \quad \eta_t \sim N(0,1) \\
\end{aligned}
$$

- State space model
- Complex! More parameters/unknowns then data points
- Estimate Using Bayesian methods

## Estimating the model on real data
 
. . .

<!-- Keen analyst wants to estimate the model straight away. 2 MCMC methods.-->
<!-- These are your posterior estimates. Which one is correct?-->
<!-- Don't care about prediction yet. Care about accurate parameter estimates -->

![]("../motivating_example/real_data_ex.png")

## Estimate model on simulated data
 
. . .

<!--Benefit of Bayesian model is that they're generative. Model can simulate data based on known parameter values.-->
<!-- Pick a value for parameteres, simulate the data, estimate the model on simulated data. See if you can capture true parameters -->
<!-- Phi not good! The model/algorithm doesn't estimate phi very well. -->
<!-- Not true. 5% chance that any random draw from the true probabiltiy dist will fall outside the 95% interval.-->

![]("../motivating_example/single_sim.png")

## Research goal
- Real data
    + Don't know much posterior estimate is correct
    + Confounding causes if convergence diagnostics fail (incorrect model, poor MCMC choice)

. . .

- Single simulation
    + Incorrect analysis may produce correct result by chance
    + Correct analysis may produce poor result by chance

. . .

**Research objective**

- Validate the computational methods used to estimate Bayesian stochastic volatility (SV) models

- Design a simulation study to check of correct posterior estimates are returned

- Compare Hamiltonian Monte Carlo with KSC (1998), which MCMC returns calibrated estimates?

# Simulation Based Calibration



## Simulation Based Calibration (SBC)
Let $\theta$ be a parameter and $y$ represent the dataset. Start with draw a sample from the prior distribution:
$$
\begin{aligned}
\theta^{sim} \sim \pi(\theta)
\end{aligned}
$$

. . .

Generate a dataset given by the prior draws.

$$
\begin{aligned}
y^{sim} \sim \pi (y|\theta^{sim})
\end{aligned}
$$

. . .

Then take draws from the posterior distribution generated by a MCMC algorithm or estimation strategy (Hamiltonian Monte Carlo or Gaussian approximation in our case).

$$
\begin{aligned}
\{\theta_1,\dots , \theta_{L}\} \sim \pi (\theta | y^{sim})
\end{aligned}
$$


## Simulation Based Calibration (SBC)
The rank statistic for a given parameter and simulation follows a discrete uniform distribution (proven in Talts et al. (2018)).

$$
\begin{aligned}
r = rank(\{\theta_1,\dots , \theta_{L}\}, \theta^{sim}) = \sum_{l=1}^{L}1[\theta_{l} < \theta^{sim}]
\end{aligned}
$$

. . .

Therefore, if the distribution of rank statistics deviates from a discrete uniform distribution, there may be evidence that our analysis or computation is not returning the correct posteriors (not calibrated). 

. . .

If Bayesian computation is well calibrated, then the posterior (credible) intervals will have sufficient coverage. That is, for any percentage interval selected (for example 90%) then 90\% of these constructed intervals will contain $\theta^{sim}$.

# Markov Chain Monte Carlo (MCMC) algorithms

## Kim, Shephard, Chib (1998)

$$
\begin{aligned}
y_t^{*} &= log(y_t^2) \\ 
&= log((\epsilon_t exp(h_t/2))^2) \\
&=  log(exp(h_t)) + log(\epsilon_t^2) \\
&= h_t + log(\epsilon_t^2)  \\
&= h_t + z_t \\
\end{aligned}
$$

Where $z_t = log(\epsilon_t^2)$ follows a log chi-squared distribution with mean -1.2704 and variance 4.93.

- Gaussian Mixture model
- Kalman Filter
- Conjugate posteriors
- Metropolis Hastings

## Hamiltonian Monte Carlo
- The Stan programming language's implementation of Hamiltonian Monte Carlo, the No-U-Turn Sampler (Hoffman & Gelman 2014), will be compared with KSC's strategy.

- Key innovations: 

    + Uses the gradients of the target posterior distribution to generate an efficient path for the sampler to explore. 

    + Allows for direct sampling of the specified stochastic volatility model

    + Own programming language. Flexibly estimate any model

# Results

## HMC (1000 Iterations)

## HMC (5000 Iterations)

## KSC (1000 Iterations)

## KSC (5000 Iterations)

# Model reparameterisation

## Reparameterised HMC (5000 Iterations)

## Reparameterised KSC (5000 Iterations)

# Discussion

# Conclusion