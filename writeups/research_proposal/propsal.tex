% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \ifXeTeX
    \usepackage{mathspec} % this also loads fontspec
  \else
    \usepackage{unicode-math} % this also loads fontspec
  \fi
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{lipsum}
\usepackage{setspace}
\onehalfspacing
\linespread{1.5}
\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Comparison of MCMC algorithms in Stochastic Volatility Models},
  pdfauthor={By Benjamin Wee (32921020); Supervised by Catherine Forbes and Lauren Kennedy},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Comparison of MCMC algorithms in Stochastic Volatility Models}
\author{By Benjamin Wee (32921020) \and Supervised by Catherine Forbes
and Lauren Kennedy}
\date{}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[interior hidden, borderline west={3pt}{0pt}{shadecolor}, sharp corners, boxrule=0pt, frame hidden, breakable, enhanced]}{\end{tcolorbox}}\fi

\hypertarget{research-goal}{%
\subsection{1. Research Goal}\label{research-goal}}

The objective of this research is to compare the computational methods
used to estimate Bayesian stochastic volatility models. A simulation
study will compare the estimation strategies detailed in the stochastic
volatility literature with Hamiltonian Monte Carlo and their ability to
sample from the model's posterior distribution. Specifically, Simulation
Based Calibration (SBC) is used to check whether these sampling
strategies are returning efficient and well calibrated posterior
estimates. Key metrics of interest are the effective sample size to
check the efficiency of the algorithm and tests of uniformity to assess
the calibration of the posteriors. This will determine which algorithm
is better at estimating stochastic volatility models based on the
efficiency and accuracy of the sampling strategy.

\hypertarget{background-and-literature-review}{%
\section{2. Background and Literature
Review}\label{background-and-literature-review}}

\hypertarget{stochastic-volatility}{%
\subsection{2.1 Stochastic Volatility}\label{stochastic-volatility}}

Stochastic volatility models are used in financial econometrics to model
the variance or volatility of returns in a financial instrument. The
return of a financial asset often exhibits patterns in the variance
which can be modeled to improve prediction and estimates of risk or tail
events. Unlike other volatility models such as ARCH or GARCH, stochastic
volatility models treat the variance as a random variable. Furthermore,
these models have a state space representation since the variance is
expressed as a latent stochastic process. State space models can be
difficult to estimate since there is a latent state for every data
observation in the time series, which means there are often more
parameters than data points. These models can be estimated using
frequent frameworks but this research will focus on Bayesian estimation
strategies.

Kim, Shephard and Chib (1998) estimate a discrete time, univariate
stochastic volatility model using Bayesian methods which is outlined
below. \(y_t\) is the mean corrected returns of some asset for equally
spaced intervals t. \(\beta\) is a constant scaling factor which is also
defined as \(exp(\mu / 2)\) representing instantaneous volatility.
\(h_t\) is log volatility, where \(h_1\) is a draw from a stationary
distribution and the state equation \(h_{t+1}\) follows a stationary
process governed by the autoregressive parameter \(\phi\) such that
\(|\phi|<1\). This autoregressive parameter represents the persistence
or ``stickiness'' of log volatility and the dispersion parameter
\(\sigma_{\eta}\) is the constant variance of the states. \(\epsilon_t\)
and \(\eta_t\) are standard normal white noise shocks and are
uncorrelated with each other.

\[
\begin{aligned}
y_t =& \space \beta exp(h_t/2) \epsilon_t \\
h_{t+1} =& \space \mu +\phi(h_t - \mu) + \sigma_{\eta} \eta_t  \\
h_1 \sim& \space normal\left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
\end{aligned}
\]

\[
\begin{aligned}
\epsilon_t \sim& \space normal(0,1) \\
\delta_t \sim& \space normal(0,1)
\end{aligned}
\]

Setting \(\beta=1\), the model can be expressed more succinctly as:

\[
\begin{aligned}
y_t \sim& \space normal(0, exp(h_t/2)) \\ 
h_1 \sim& \space normal \left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
h_{t+1} \sim& \space normal(\mu +\phi(h_t - \mu) , \sigma_{\eta}^2), \space\space t\neq 1\\ 
\end{aligned}
\]

Priors for the static parameters are defined below with conjugate priors
on \(\mu\) and \(\sigma^2\):

\[
\begin{aligned}
\mu \sim& \space normal(0, 10^2) \\
\sigma_{\eta}^2 \sim& \space IG(5/2, (0.01\times 5) / 2) \\
\phi^{\ast} \sim& \space beta(20, 1.5) \\
\phi &=  2\phi^{\ast} - 1
\end{aligned}
\]

The prior on \(\phi\) is a ``stretched'' beta distribution. This is a
beta distribution (as defined on the parameter \(\phi^*\)) which has
been transformed to have support (-1, 1).

\hypertarget{estimation-strategy}{%
\subsubsection{Estimation strategy}\label{estimation-strategy}}

Kim, Shephard and Chib (KSC) sample the posteriors of the stochastic
volatility model using a mix of conjugate posterior distributions,
Metropolis Hastings and the Kalman Filter and smoother\footnote{In this
  research the exact software to apply the simulation smoother is
  unavailable, so a more recent simulation smoother is used which is
  based on the software written by the same author.} (de Jong and
Shephard, 1995).

The standard Kalman Filter and smoother is used to compute the posterior
distribution over the latent states. This requires the state and
measurement equations to be linear and conditionally Gaussian. Since the
relationship between \(y_t\) and \(h_t\) in the measurement equation is
not linear, a transformation is applied by squaring and taking the log
of \(y_t\).

\[
\begin{aligned}
y_t^{*} &= log(y_t^2) \\ 
&= log((\epsilon_t exp(h_t/2))^2) \\
&=  log(exp(h_t)) + log(\epsilon_t^2) \\
&= h_t + log(\epsilon_t^2)  \\
&= h_t + z_t \\
\end{aligned}
\]

Where \(z_t = log(\epsilon_t^2)\) follows a log chi-squared distribution
with mean -1.2704 and variance 4.93. The relationship between \(y_t\)
and \(h_t\) is now linear; however, the error is not Gaussian. Since it
is not simple to sample from this parameterisation of the model, KSC use
a mixture of Gaussians to \textbf{approximate} the first 4 moments of
the log chi squared distribution. This is defined by:

\[
\begin{aligned}
f(z_t) = \sum_{i=1}^{K} q_if_N(z_i|m_i-1.2704, \nu_i^2)
\end{aligned}
\]

Where K is the mixture of normal densities \(f_N\), component
probabilities \(q_i\), mean \(m_i-1.2704\) and variance \(\nu_i^2\).
These parameters were selected using moment matching where they found 7
normal densities with varying mean and variance parameters best
approximated the log chi squared moments. These parameters and weights
can be found in Appendix A.

The model can be sampled via the Kalman Filter and simulation smoother
since the model is now linear and conditionally gaussian. The static
parameters \(\mu\) and \(\sigma^2\) are sampled directly from their
conjugate posterior distributions whereas \(\phi\) is sampled via a
Metropolis Hastings accept/reject procedure. The details can be around
in Appendix B.

\hypertarget{hamiltoninan-monte-carlo}{%
\subsection{2.2 Hamiltoninan Monte
Carlo}\label{hamiltoninan-monte-carlo}}

Since the paper was written, new MCMC algorithms have been developed and
enabled the estimation of richer and more complicated models.
Specifically, Hamiltonian Monte Carlo is a MCMC algorithm which has
become widely available for efficiently sampling from sophisticated
models. Hamiltonian Monte Carlo, originally called Hybrid Monte Carlo,
was developed in the physics literature (Duane et al.~1987) before being
applied in the statistics literature by Radford Neal through his works
in Bayesian Neural Networks (Neal, 1995) and statistical computing (Neal
2011). The algorithm has since become widely available through open
source development projects such as Stan (Stan Development Team, 2023)
and PyMC (2023).

The key innovation of Hamiltonian Monte Carlo is using the gradients of
the target posterior distribution to generate an efficient path for the
sampler to explore. Unlike Random Walk Metropolis Hastings, it takes
advantage of the geometry of the posterior to determine its next
proposal step. A comprehensive explanation of the sampler is beyond the
scope of this research and can be found in the above references.

The Stan programming language's implementation of Hamiltonian Monte
Carlo will be used for this study. Stan's default algorithm, the
No-U-Turn Sampler (Hoffman \& Gelman 2014), allows for direct sampling
of the specified stochastic volatility model. Hamiltonian Monte Carlo
allows for sampling of the generative model and can flexibly handle
complicated likelihood functions. This approach will also use the same
priors as specified in the Gaussian mixture approximation.

\hypertarget{research-contribution}{%
\subsection{2.3 Research contribution}\label{research-contribution}}

The goal of this research is to determine which algorithm is better at
estimating the discrete time stochastic volatility model (conditional on
the same priors). My main contribution is to design a simulation study
that compares these algorithms and determines which sampling strategy is
better based on the calibration of posterior estimates and the
efficiency of the MCMC sampler.

\hypertarget{methodology}{%
\section{3. Methodology}\label{methodology}}

\hypertarget{design}{%
\subsection{3.1 Design}\label{design}}

Simulation Based Calibration (SBC) is a technique that checks the
calibration of posterior estimates generated by MCMC algorithms. SBC is
conducted by comparing the distribution of rank statistics to the
discrete uniform distribution which arises when an algorithm is
correctly calibrated. The procedure starts by taking draws from the
prior distribution and creating datasets implied by each draw. Rank
statistics are then calculated on the posterior samples conditional on
the simulated data.

To illustrate this procedure, let \(\theta\) be a parameter and \(y\)
represent the dataset. Start with a single draw from the prior
distribution: \[
\begin{aligned}
\theta^{sim} \sim \pi(\theta)
\end{aligned}
\]

Generate a dataset given by the prior draw.

\[
\begin{aligned}
y^{sim} \sim \pi (y|\theta^{sim})
\end{aligned}
\]

Then take draws from the posterior distribution generated by a MCMC
algorithm or estimation strategy (Hamiltonian Monte Carlo or KSC)
conditional on this dataset.

\[
\begin{aligned}
\{\theta_1,\dots , \theta_{L}\} \sim \pi (\theta | y^{sim})
\end{aligned}
\]

A key result is that the posterior sample
\(\{\theta_1,\dots , \theta_{L}\}\) will share the same distribution as
the prior samples \(\theta^{sim}\). This is implied by the following
expression:

\[
\begin{aligned}
\pi(\theta) &= \int \pi(\theta|y^{sim}) \pi(y^{sim}|\theta^{sim}) \pi(\theta^{sim})dy^{sim} d\theta^{sim} \\
&= \int \pi(\theta|y^{sim}) \pi(y^{sim},\theta^{sim}) dy^{sim} d\theta^{sim}
\end{aligned}
\]

That is, the posterior averaged over the joint distribution follows the
same distribution as the prior. The procedure of generating posterior
samples implicitly performs this integral since the expression on the
right of the integral is proportional to the prior density. Therefore,
any deviation of the posterior samples from the prior distribution
suggests that the sampling methodology is not producing the correct
posteriors.

If the posterior samples follows the prior distribution, the rank
statistic for a given parameter follows a discrete uniform
distribution\footnote{Proof of this result in Talts et al (2018).}. The
rank statistic is defined as:

\[
\begin{aligned}
r = rank(\{\theta_1,\dots , \theta_{L}\}, \theta^{sim}) = \sum_{l=1}^{L}1[\theta_{l} < \theta^{sim}]
\end{aligned}
\]

This completes one iteration of SBC. To complete the algorithm, multiple
iterations are run and the rank statistics are calculated for each
parameter. The resulting rank statistics are compared to the discrete
uniform distribution to determine if any problematic features exist.

If the computation is well calibrated and the rank statistics follow a
discrete uniform distribution, then the posterior credible intervals
have sufficient coverage. That is, one way to describe calibration is:
for any percentage interval selected over the posterior samples (for
example 90\%) then there is a 90\% chance that \(\theta^{sim}\) falls
within this interval. Another way of saying this is a Bayesian analysis
is well calibrated if a 90\% credible interval contains the true
parameter in 90\% of the SBC iterations.

\hypertarget{implementation}{%
\subsection{3.2 Implementation}\label{implementation}}

SBC for Hamiltonian Monte Carlo and the Gaussian approximation are
performed with 1000 simulation iterations, 1000 observations in each
dataset, and post warmup/burnin samples of 999 (which gives 1000
possible rank statistics). This algorithm is summarised below:

\textbf{1) for sim in 1000 iterations:}\\
\hspace*{0.333em}\hspace*{0.333em}2) Draw from prior:
\(\theta^{sim}\sim\pi (\theta)\)\\
\hspace*{0.333em}\hspace*{0.333em}3) Simulate dataset with 1000
observations: \(y^{sim} \sim p(y|\theta^{sim})\)\\
\hspace*{0.333em}\hspace*{0.333em}4) Draw 999 posterior samples (post
warmup) \(\{\theta_1,\dots , \theta_{L}\} \sim p(\theta | y^{sim})\)\\
\hspace*{0.333em}\hspace*{0.333em}5) Compute rank statistics
\(r = rank(\{\theta_1,\dots , \theta_{L}\}, \theta^{sim})\)

\hypertarget{metrics}{%
\subsection{3.3 Metrics}\label{metrics}}

The key metrics and diagnostics to compare the performance of these
methods are the effective sample size (ESS), rank statistics, and
chi-squared test statistics.

ESS measures the efficiency of the MCMC sampler. It calculates the
number of (effectively) independent draws from the posterior draws
generated by a Markov chain. A poor ESS can arise from high
autocorrelation in the Markov chain which leads to highly dependent
samples. An efficient MCMC algorithm takes less resources (for example,
time and number of draws) to get a representative sample of the target
distribution. If a MCMC algorithm possesses higher ESS for the majority
of its parameters (relative to another strategy), then we may conclude
that this method is a more efficient sampler (conditional on the model).

Rank statistics as described in the SBC section are used to evaluate the
calibration of a posterior. Histograms will be used to evaluate the
distribution of rank statistics. If a posterior is well calibrated then
it is expected that the histogram is uniform.

A drawback of this approach is there are more parameters than data
points in this model. An alternative to visually checking for uniformity
of all the parameters is to calculate the chi squared statistics for the
counts in each histogram bin. Let \(b_j\) be the number of counts and
\(e_j\) the expected count in bin \(j\). Then the chi squared statistic
is given by:

\[
\begin{aligned}
\chi^2 = \sum_{j=1}^J \frac{(b_{j} - e_{j})^2}{e_j}
\end{aligned}
\]

Chi squared statistics can be used to test for uniformity and thus
calibration of all the parameters in the model.

\hypertarget{preliminary-results}{%
\section{4. Preliminary Results}\label{preliminary-results}}

Figure 1 shows the distribution of rank statistics for Hamiltonian Monte
Carlo, with the horizontal black line at 50 representing the discrete
uniform distribution. The preliminary figures focus on the static
parameters and arbitrarily chosen states. Phi and sigma\_sqd look
relatively uniform. However mu and h.995 (the 995th state parameter)
have some lumpiness which may suggest a lack of calibration.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{../../simulation_output/sbc_ncp_ksc_priors_0.999_adapt_delta_premade_datasets_r7_1000_iterations/static_state_hist.png}

}

\caption{Distribution of parameter ranks for HMC algorithm (1000
iterations)}

\end{figure}

A drawback of SBC is that any deviation in uniformity from a calibrated
analysis may be due to noisy estimates from small samples. Figure 2
shows more consistent uniform behaviour from all parameters after
increasing the number of iterations from 1000 to 5000. This suggests
that Hamiltonian Monte Carlo and the overall analysis is well
calibrated.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{../../simulation_output/sbc_ncp_ksc_priors_0.999_adapt_delta_premade_datasets_r6_5000_iterations/static_state_hist.png}

}

\caption{Distribution of parameter ranks for HMC algorithm (5000
iterations)}

\end{figure}

Figure 3 shows results for the Gaussian approximation. The distribution
of the rank statistics deviates heavily from the expected discrete
uniform distribution, with various peaks at each end of the histogram as
well as peaks in the middle. A reason for this miscalibration may be due
to the Gaussian mixture only approximating the log chi squared
distribution. Hamiltonian Monte Carlo on the other hand, samples
directly from the target model which more closely represents the data
generating process.

This result may also be due to the relatively small number of posterior
samples drawn from the model. The MCMC sampler may not have converged
yet which may explain the deviations from uniformity. A follow up step
is to compare the ESS of the two methods and to increase the length of
the MCMC chain to ensure the ESS is approximately the same before
comparing histograms.

\begin{figure}[H]

{\centering \includegraphics[width=0.75\textwidth,height=\textheight]{../../simulation_output/sbc_cp_ksc_model_cp_dgf_r1/static_state_hist.png}

}

\caption{Distribution of parameter ranks for Gaussian mixture model
(1000 iterations)}

\end{figure}

\hypertarget{conclusion-and-next-steps}{%
\section{5. Conclusion and next steps}\label{conclusion-and-next-steps}}

There are two key extensions to the simulation results worth exploring.

\textbf{1) Metropolis Hastings reweighting for KSC model}

Kim, Shephard and Chib apply importance sampling weights to correct for
the approximation error of the Gaussian mixture model. According to the
authors, the correction ensures that samples are from the exact
posterior density. The next step is to perform this correction and
evaluate the SBC results. I'd expect the Gaussian mixture approximation
diagnostics to improve after this correction.

\textbf{2) Centered and non centered parameterisation}

The parameterisation of a model can affect the performance of a sampler.
This can cause significant problems in getting unbiased MCMC samples for
complex models. An example of this is Neal's funnel (Neal 2003) which
arises in hierarchical models which causes performance issues in the
Hamiltonian Monte Carlo sampler. Rewriting the model with non centered
parameterisation significantly improves the performance of MCMC. Another
consideration for this research is to consider both centered and non
centered parameterisations of the stochastic volatility model to see if
there are any improvements to the simulation diagnostics.

The models and parameterisations for this research are summarised in the
below table:

\begin{figure}[H]

{\centering \includegraphics{model_table.jpg}

}

\caption{Model matrix}

\end{figure}

\hypertarget{plan}{%
\subsection{Plan}\label{plan}}

Week 7: Reflect and integrate feedback from presentation into plan for
the rest of the semester

Week 8: Finish off any simulation or modelling not yet complete (listed
in above matrix)

Week 9: Write up results and have a first draft ready for feedback

Week 10-11: Action any feedback, prepare for final submission and
presentation

Week 12: Submit

\newpage{}

\hypertarget{references}{%
\section{References}\label{references}}

Abril-Pla O, Andreani V, Carroll C, Dong L, Fonnesbeck CJ, Kochurov M,
Kumar R, Lao J, Luhmann CC, Martin OA, Osthege M, Vieira R, Wiecki T,
Zinkov R. (2023) PyMC: a modern, and comprehensive probabilistic
programming framework in Python.

Duane, Simon; Kennedy, Anthony D.; Pendleton, Brian J.; Roweth, Duncan
(1987). ``Hybrid Monte Carlo''. Physics Letters B. 195 (2): 216--222.

Hoffman, M. D., \& Gelman, A. (2014). The No-U-Turn sampler: adaptively
setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res.,
15(1), 1593-1623.

Kim, Sangjoon, Neil Shephard, and Siddhartha Chib. 1998. ``Stochastic
Volatility: Likelihood Inference and Comparison with ARCH Models.''
Review of Economic Studies 65: 361--93.

Neal, R.M. (1995) Bayesian Learning for Neural Networks. Ph.D.~Thesis,
Graduate Department of Computer Science, University of Toronto, Toronto.

Neal, R. M. (2003). Slice Sampling. The Annals of Statistics, 31(3),
705--741. http://www.jstor.org/stable/3448413

Neal, R.M. (2011) MCMC Using Hamiltonian Dynamics. In Handbook of Markov
Chain Mone Carlo, CRC Press, New York

Stan Development Team. 2023. Stan Modeling Language Users Guide and
Reference Manual, 2.32. https://mc-stan.org

Talts, S., Betancourt, M., Simpson, D., Vehtari, A., \& Gelman, A.
(2018). Validating Bayesian inference algorithms with simulation-based
calibration. arXiv preprint arXiv:1804.06788.

\newpage{}

\hypertarget{appendix-a-mixture-gaussian-weights}{%
\section{Appendix A: Mixture Gaussian
weights}\label{appendix-a-mixture-gaussian-weights}}

\begin{longtable}[]{@{}llll@{}}
\toprule\noalign{}
\(\omega\) & \(Pr(\omega = i)\) & \(m_i\) & \(\nu^2_i\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.00730 & -10.12999 & 5.79596 \\
2 & 0.10556 & -3.97281 & 2.61369 \\
3 & 0.00002 & -8.56686 & 5.17950 \\
4 & 0.04395 & 2.77786 & 0.16735 \\
5 & 0.34001 & 0.61942 & 0.64009 \\
6 & 0.24566 & 1.79518 & 0.34023 \\
7 & 0.25750 & -1.08819 & 1.26261 \\
\end{longtable}

\newpage{}

\hypertarget{appendix-b-sampling-from-mixture-of-gaussians}{%
\section{Appendix B: Sampling from mixture of
Gaussians}\label{appendix-b-sampling-from-mixture-of-gaussians}}

\hypertarget{conjugate-posterior-distributions}{%
\subsection{Conjugate posterior
distributions}\label{conjugate-posterior-distributions}}

\textbf{Sampling} \(\boldsymbol{\sigma_{\eta}^2}\)

Inverse gamma conjugate posterior distribution:

\[
\sigma^2_{\eta} | y,h,\phi,\mu \sim IG \Bigl\{\frac{n+\sigma_r}{2}, \frac{0.05 + (h_1 - \mu)^2 (1 - \phi^2) + \sum_{t=1}^{n-1}((h_{t+1} - \mu) - \phi(h_t - \mu))^2}{2}\Bigr\}
\]

\textbf{Sampling} \(\boldsymbol{\mu}\)

Gaussian conjugate posterior distribution:

\[
\mu | h,\phi,\sigma^2_{\eta}  \sim N(\hat{\mu}, \sigma^2_{\mu})
\]

Where

\[
\begin{aligned}
\hat{\mu} &= \sigma^2_{\mu} \Bigl\{\frac{(1-\phi^2)}{\sigma_{\eta}^2}h_1 +\frac{(1-\phi^2)}{\sigma_{\eta}^2} \sum_{t=1}^{n-1} (h_{t+1} - \phi h_t)\Bigr\} \\
\sigma^2_{\mu} &= \sigma^2_{\eta} \{(n-1)(1-\phi)^2 + (1-\phi^2)\}^{-1}
\end{aligned}
\]

\textbf{Sampling} \(\boldsymbol{\phi}\)

Metropolis Hastings accept/reject procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Generate proposal \(\phi^\ast\) from \(N(\hat{\phi}, V_{\phi})\) where
  \(\hat{\phi} = \frac{\sum_{t=1}^{n-1} (h_{t+1} - \mu)(h_t - \mu)}{\sum_{t=1}^{n-1} (h_t - \mu)^2}\)
  and
  \(V_{\phi} = \sigma^2_{\eta} \{\sum_{t=1}^{n-1} (h_t - \mu)^2\}^{-1}\)
\item
  Accept proposal as \(\phi^{(i)}\) with probability
  \(e^{\{g(\phi^\ast) - g(\phi^{(i-1)}\}}\) such that
  \(g(\phi) = log (\pi (\phi)) - \frac {(h_t - \mu)^2 (1-\phi^2)}{2 \sigma_{\eta}^2} + \frac{1}{2} log (1-\phi^2)\)
\end{enumerate}

\textbf{Sampling mixture density:}

Rewrite mixture density with respect to a indicator variable \(s_t\)

\[
\begin{aligned}
&z_t | s_t = i \sim N(m_i - 1.2704, \nu^2) \\
&Pr(s_t = i) = q_i
\end{aligned}
\]

Sample \(s_t\) from probability mass function:

\[
\begin{aligned}
Pr(s_t = i | y_t^{\ast}, h_t) \propto q_i f_N(y_t^{\ast} | h_t + m_t - 1.2704, \nu^2)
\end{aligned}
\]

\textbf{Sampling steps}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Set initial values \(s\), \(\phi\), \(\sigma^2_{\eta}\) and \(\mu\)
\item
  Sample \(h\) from \(h|y^{\ast}, s, \phi, \sigma^2_{\eta}, \mu\)
\item
  Sample s from \(s|y^{\ast}, h\)
\item
  Sample \(\mu|y^{\ast}, s, \phi, \sigma^2_{\eta}, h\) according to
  conjugate posterior distribution
\item
  Sample \(\sigma_{\eta}^2|y^{\ast}, s, \phi, \mu, h\) according to
  conjugate posterior distribution
\item
  Sample \(\phi|y^{\ast}, s, \mu, \sigma^2_{\eta}, h\) using metropolis
  hastings
\end{enumerate}



\end{document}
