---
title: "Comparison of MCMC algorithms in Stochastic Volatility Models"
format: 
    beamer:
        incremental: true
        header-includes: |
            \setbeamertemplate{itemize items}[circle]
author: "By Benjamin Wee"
institute: "Supervised by Catherine Forbes and Lauren Kennedy"
---

## Background
- Variety of Bayesian estimation strategies for different types of Stochastic Volatility models using classical Markov Chain Monte Carlo (MCMC) algorithms

- Since then, more advanced algorithms such as Hamiltoninan Monte Carlo has enabled the estimation of richer and more complicated models

- However, as the developments of these tools enable the estimation of more complicated models so do the risk of mistakes and the need to validate the output of our algorithms

- How do we ensure the validity of inferences from our algorithms across different estimation strategies?

<!-- What is the fundamental difference between convergence diagnostics, bayes factors with SBC? Check the fit of the model?-->

## Research Goal

Validate the computational methods used to estimate Bayesian stochastic volatility (SV) models

. . .

- 3 key components
    + Kim, Sherphard and Chib (1998) SV model
    + Hamiltonian Monte Carlo
    + Simulation Based Calibration to validate MCMC computation

## Stochastic Volatility (SV) model
KSC (1998) estimate a univariate discrete time SV model which models the variance as a latent stochastic process.

. . .

$$
\begin{aligned}
y_t =& \space \beta exp(h_t/2) \epsilon_t \\
h_{t+1} =& \space \mu +\phi(h_t - \mu) + \sigma_{\eta} \eta_t  \\
h_1 \sim& \space normal\left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
\epsilon_t \sim &N(0,1) \space\space\space \eta_t \sim N(0,1)
\end{aligned}
$$

. . .

:::{.nonincremental}
- $y_t$ is mean corrected returns for equally spaced intervals $t$

- $\mu$ is the "average" log volatility

- $\beta = exp(\mu / 2)$ constant scaling factor (set to 1)

- $h_t$ is log volatility where $h_1$ is a draw from a stationary distribution and the state equation $h_{t+1}$ is a stationary process governed by the autoregressive parameter $\phi$ where $|\phi|<1$.

:::

## Kim Sherphard Chib (1998) Estimation strategy
:::{.nonincremental}
- Estimation of states using a Kalman filter, however, this requires the model to be linear and Gaussian. 

. . .

- Log and square both sides to linearise the model

. . .

$$
\begin{aligned}
y_t^{*} &= log(y_t^2) \\ 
&= log((\epsilon_t exp(h_t/2))^2) \\
&=  log(exp(h_t)) + log(\epsilon_t^2) \\
&= h_t + log(\epsilon_t^2)  \\
&= h_t + z_t \\
\end{aligned}
$$

Where $z_t = log(\epsilon_t^2)$ follows a log chi-squared distribution with mean -1.2704 and variance 4.93.

:::

## Kim Sherphard Chib (1998) Estimation strategy
Model is now linear but not Gaussian. KSC use a mixture of Gaussians to **approximate** the first 4 moments of the log chi squared distribution through moment matching. This is defined by:

$$
\begin{aligned}
f(z_t) = \sum_{i=1}^{K} q_if_N(z_i|m_i-1.2704, \nu_i^2)
\end{aligned}
$$

K is the mixture of 7 normal densities $f_N$, component probabilities $q_i$, mean $m_i-1.2704$ and variance $\nu_i^2$.

. . .

The static parameters $\mu$ and $\sigma^2$ are sampled directly from their conjugate posterior distributions where as $\phi$ is sampled via a metropolis hastings accept/reject procedure.

## Hamiltonian Monte Carlo (HMC)
- Developed in the physics literature (Duane et al. 1987) before being applied in the statistics literature by Radford Neal through his works in Bayesian Neural Networks (Nael, 1995) and statistical computing (Neal 2011). 

- Key innovation: using the gradients of the target posterior distribution to generate an efficient path for the sampler to explore. It takes advantage of the geometry of the posterior to determine its next step.

- The Stan programming language's implementation of Hamiltonian Monte Carlo will be used for this study. Stan's default algorithm, the No-U-Turn Sampler (Hoffman & Gelman 2014), allows for direct sampling of the specified stochastic volatility model.


## Simulation Based Calibration (SBC)
<!-- Probably need to talk about what calibration means-->
- The procedure starts by taking draws from the prior distribution and creating datasets implied by each draw. 

- Rank statistics are then calculated on the posterior samples estimated from the simulated data. 

- SBC compares the distribution of rank statistics to the discrete uniform distribution which arises when an algorithm is correctly calibrated.

- If the computation is well calibrated and the samples follow a discrete uniform distribution, then the posterior (credible) intervals will have sufficient coverage. That is, for any percentage interval selected (for example 90%) then there is 90\% chance that any $\theta^{sim}$ falls within this interval.


## Simulation Based Calibration (SBC)
Let $\theta$ be a parameter and $y$ represent the dataset. Start with draw a sample from the prior distribution:
$$
\begin{aligned}
\theta^{sim} \sim \pi(\theta)
\end{aligned}
$$

. . .

Generate a dataset given by the prior draws.

$$
\begin{aligned}
y^{sim} \sim \pi (y|\theta^{sim})
\end{aligned}
$$

. . .

Then take draws from the posterior distribution generated by a MCMC algorithm or estimation strategy (Hamiltonian Monte Carlo or Gaussian approximation in our case).

$$
\begin{aligned}
\{\theta_1,\dots , \theta_{L}\} \sim \pi (\theta | y^{sim})
\end{aligned}
$$


## Simulation Based Calibration (SBC)
A key result from this procedure is that the posterior sample $\{\theta_1,\dots , \theta_{L}\}$ will share the same distribution as the prior samples $\theta^{sim}$. This is implied by the following expression (and proven in Talts et al. (2018)):

$$
\begin{aligned}
\pi(\theta) &= \int \pi(\theta|y^{sim}) \pi(y^{sim}|\theta^{sim}) \pi(\theta^{sim})dy^{sim} d\theta^{sim} \\
&= \int \pi(\theta|y^{sim}) \pi(y^{sim},\theta^{sim}) dy^{sim} d\theta^{sim}
\end{aligned}
$$

. . .

That is, the posterior averaged over the joint distribution follows the same distribution as the prior. Therefore, any deviation of the posterior samples from the prior distribution suggests an error - either in computation, code or overall analysis. 

<!-- Algorithm, computation, model strategy on average fails to estimate the parameters of a known data generating process. Whether or not the model is fit for a particular use case is not answered here. It's whether the model can capture the _assumed_ structure of the proposed DGP (we can't tell if this DGP is "correct" in the real world) -->


## HMC Results

![1000 Iterations of SBC (HMC)](../../simulation_output/sbc_ncp_ksc_priors_0.999_adapt_delta_premade_datasets_r7_1000_iterations/static_state_hist.png){width=80%}

## HMC Results

![5000 Iterations of SBC (HMC)](../../simulation_output/sbc_ncp_ksc_priors_0.999_adapt_delta_premade_datasets_r6_5000_iterations/static_state_hist.png){width=75%}

## Gaussian mixture Results

![1000 Iterations of SBC (KSC)](../../simulation_output/sbc_cp_ksc_model_cp_dgf_r1/static_state_hist.png){width=75%}

## Next steps

- Increasing number of posterior draws for KSC model
    + MCMC may not have converged onto target distribution

- Different model parameterisation
    + Performance of a sampler may be sensititive to the shape of the posterior density

- KSC's correction using importance sampling weights (MH correction step)
    + Reweights samples from approximate distribution such that reweighted draws come from the correct distribution
