---
title: "Comparison of MCMC algorithms in Stochastic Volatility Models"
format: 
    beamer:
        incremental: true
        header-includes: |
            \setbeamertemplate{itemize items}[circle]
author: "By Benjamin Wee"
institute: "Supervised by Catherine Forbes and Lauren Kennedy"
---

# What is Stochastic Volatility?

## Log returns S&P 500
![]("returns.png")

## Stochastic Volatility (SV) model

$$
\begin{aligned}
y_t =& \space \beta e^{h_t/2} \epsilon_t \\
h_{t+1} =& \space \mu +\phi(h_t - \mu) + \sigma_{\eta} \eta_t  \\
h_1 \sim&  N\left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
\epsilon_t \sim& N(0,1) \quad \eta_t \sim N(0,1) \\
\end{aligned}
$$

- $y_t$ is mean corrected returns for equally spaced intervals $t$

- $\beta = e^{\mu / 2}$ constant scaling factor (set to 1)

- $h_t$ is log volatility where $h_1$ is a draw from a stationary distribution  

- State equation $h_{t+1}$ is a stationary process where $\mu$ is the "average" log volatility and determined by the autoregressive parameter $\phi$ where $|\phi|<1$.

## Research Goal

Validate the computational methods used to estimate Bayesian stochastic volatility (SV) models

. . .

- Compare the Kim, Shephard and Chib (1998) estimation strategy with Hamiltonian Monte Carlo

- Simulation Based Calibration to validate MCMC computation

# Estimation strategy

## Stochastic Volatility (SV) model
KSC (1998) estimate a univariate discrete time SV model which models the variance as a latent stochastic process.

$$
\begin{aligned}
y_t =& \space \beta e^{h_t/2} \epsilon_t \\
h_{t+1} =& \space \mu +\phi(h_t - \mu) + \sigma_{\eta} \eta_t  \\
h_1 \sim& N\left(\mu, \frac{\sigma_{\eta}^2}{1-\phi^2}\right) \\
\epsilon_t \sim& N(0,1) \quad \eta_t \sim N(0,1) \\
\end{aligned}
$$

. . .

With priors:
$$
\begin{aligned}
\mu \sim& \space N(0, 10^2) \\
\sigma_{\eta}^2 \sim& \space IG(5/2, (0.01\times 5) / 2) \\
\phi^{\ast} \sim& \space beta(20, 1.5) \\
\phi =& 2\phi^{\ast} - 1
\end{aligned}
$$

## Kim Shephard Chib (1998)
:::{.nonincremental}
- Estimation of states using a standard Kalman filter, however, this requires the model to be linear and Gaussian. 

. . .

- Log and square both sides to linearise the model

. . .

$$
\begin{aligned}
y_t^{*} &= log(y_t^2) \\ 
&= log((\epsilon_t exp(h_t/2))^2) \\
&=  log(exp(h_t)) + log(\epsilon_t^2) \\
&= h_t + log(\epsilon_t^2)  \\
&= h_t + z_t \\
\end{aligned}
$$

Where $z_t = log(\epsilon_t^2)$ follows a log chi-squared distribution with mean -1.2704 and variance 4.93.

:::

## Kim Shephard Chib (1998)
Model is now linear but not Gaussian. KSC use a mixture of Gaussians to **approximate** the first 4 moments of the log chi squared distribution through moment matching. This is defined by:

$$
\begin{aligned}
f(z_t) = \sum_{i=1}^{K} q_if_N(z_i|m_i-1.2704, \nu_i^2)
\end{aligned}
$$

K is the mixture of 7 normal densities $f_N$, component probabilities $q_i$, mean $m_i-1.2704$ and variance $\nu_i^2$.

. . .

The static parameters $\mu$ and $\sigma^2$ are sampled directly from their conjugate posterior distributions where as $\phi$ is sampled via a metropolis hastings accept/reject procedure.

## Hamiltonian Monte Carlo (HMC)
- Key innovations: 

    + Uses the gradients of the target posterior distribution to generate an efficient path for the sampler to explore. 

    + Imagine the parameter is a marble rolling in random directions inside a bowl (negative log transformation of joint posterior)

    + Allows for direct sampling of the specified stochastic volatility model
    
- The Stan programming language's implementation of Hamiltonian Monte Carlo, the No-U-Turn Sampler (Hoffman & Gelman 2014), will be compared with KSC's strategy.

# Simulation Based Calibration (SBC)

## Simulation Based Calibration (SBC)
Let $\theta$ be a parameter and $y$ represent the dataset. Start with draw a sample from the prior distribution:
$$
\begin{aligned}
\theta^{sim} \sim \pi(\theta)
\end{aligned}
$$

. . .

Generate a dataset given by the prior draws.

$$
\begin{aligned}
y^{sim} \sim \pi (y|\theta^{sim})
\end{aligned}
$$

. . .

Then take draws from the posterior distribution generated by a MCMC algorithm or estimation strategy (Hamiltonian Monte Carlo or Gaussian approximation in our case).

$$
\begin{aligned}
\{\theta_1,\dots , \theta_{L}\} \sim \pi (\theta | y^{sim})
\end{aligned}
$$


## Simulation Based Calibration (SBC)
The rank statistic for a given parameter and simulation follows a discrete uniform distribution (proven in Talts et al. (2018)).

$$
\begin{aligned}
r = rank(\{\theta_1,\dots , \theta_{L}\}, \theta^{sim}) = \sum_{l=1}^{L}1[\theta_{l} < \theta^{sim}]
\end{aligned}
$$

. . .

Therefore, if the distribution of rank statistics deviates from a discrete uniform distribution, there may be evidence that our analysis or computation is not returning the correct posteriors (not calibrated). 

. . .

If Bayesian computation is well calibrated, then the posterior (credible) intervals will have sufficient coverage. That is, for any percentage interval selected (for example 90%) then 90\% of these constructed intervals will contain $\theta^{sim}$.

<!-- Algorithm, computation, model strategy on average fails to estimate the parameters of a known data generating process. Whether or not the model is fit for a particular use case is not answered here. It's whether the model can capture the _assumed_ structure of the proposed DGP (we can't tell if this DGP is "correct" in the real world) -->

# Results

## HMC Results (1000 iterations)

![](../../simulation_output/sbc_ncp_ksc_priors_0.999_adapt_delta_premade_datasets_r7_1000_iterations/static_state_hist.png){fig-align="center" height=90%}

## HMC Results (5000 iterations)

![](../../simulation_output/sbc_ncp_ksc_priors_0.999_adapt_delta_premade_datasets_r6_5000_iterations/static_state_hist.png){fig-align="center" height=90%}

## Gaussian mixture Results (1000 iterations)

![](../../simulation_output/sbc_cp_ksc_model_cp_dgf_r1/static_state_hist.png){fig-align="center" height=90%}

## Next steps
:::{.nonincremental}
- Increasing number of posterior draws for KSC model
    + MCMC may not have converged onto target distribution

- Different model parameterisation
    + Performance of a sampler may be sensititive to the shape of the posterior density

- KSC's correction using importance sampling weights
    + Reweight samples from Gaussian mixture so that the new draws come from the correct distribution

:::

# Appendix

## Simulation Based Calibration (SBC)
A key result from this procedure is that the posterior sample $\{\theta_1,\dots , \theta_{L}\}$ will share the same distribution as the prior samples.

$$
\begin{aligned}
\pi(\theta) &= \int \pi(\theta|y^{sim}) \pi(y^{sim}|\theta^{sim}) \pi(\theta^{sim})dy^{sim} d\theta^{sim}  \\
&= \int \pi(\theta|y^{sim}) \pi(y^{sim},\theta^{sim}) dy^{sim} d\theta^{sim}
\end{aligned}
$$

That is, the posterior averaged over the joint distribution follows the same distribution as the prior. The rank statistic for a given parameter and simulation follows a discrete uniform distribution (proven in Talts et al. (2018)).
